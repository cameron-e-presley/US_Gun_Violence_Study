{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# The Violence Project Mass Shooter Database Review :  EDA + Categorical Modeling\n",
    "\n",
    "Author:  Cameron Presley\n",
    "\n",
    "Version : 2022-07019\n",
    "\n",
    "email : cameron@cameron-presley.com\n",
    "\n",
    "Dataset Filename : Violence-Project-Mass-Shooter-Database-Version-5-May-2022.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background and Context\n",
    "\n",
    "Insert narrative and main problem we are trying to solve\n",
    "\n",
    "# Objective: \n",
    "\n",
    "Identify the main objective\n",
    "\n",
    "\n",
    "# Dataset:  \n",
    "\n",
    "Review data set\n",
    "\n",
    "\n",
    "# Data Description: \n",
    "\n",
    "Dataset description\n",
    "\n",
    "\n",
    "# Deliverables\n",
    "\n",
    "Project deliverables\n",
    "\n",
    "\n",
    "# Submission / Final Output Comments and Notes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Outline - Sections\n",
    "\n",
    "\n",
    "- <a href = #link1>1.  Exploratory Data Analysis </a>\n",
    "\n",
    "    - Univariate analysis\n",
    "    - Bivariate analysis\n",
    "    - Multivariate analysis\n",
    "\n",
    "- <a href = #link2>2.  Insights based on EDA </a>\n",
    "\n",
    "- <a href = #link3>3.  Data Pre-processing </a>\n",
    "\n",
    "    - Prepare the data for analysis \n",
    "    - Missing value Treatment, Outlier Detection/Treatment, Feature Engineering, Data Prep, Model Split as required\n",
    "\n",
    "\n",
    "- <a href = #link4>4.  insert model dev + performance measurement\n",
    "\n",
    "- <a href = #link7>5.  PCA\n",
    "\n",
    "- <a href = #link5>6.  Actionable Insights & Business Recommendations </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In standard Markdown, place an anchor <a name=\"abcd\"></a> \n",
    "# where you want to link to and refer to it on the same page by [link text](#abcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import libraries and avoid warnings\n",
    "\n",
    "!pip3 install yellowbrick\n",
    "!pip3 install openpyxl\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "#import libraries for working with arrays, dataframes, and performing linear algebra operations\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "#library needed to read .xlsx files\n",
    "import xlrd \n",
    "\n",
    "#data visualization librariers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#statistical analysis libraries\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.formula.api import ols # for ANOVA\n",
    "from statsmodels.stats.anova import anova_lm # for ANOVA\n",
    "from scipy.stats import chi2_contingency # for CHI SQUARE\n",
    "from scipy.stats import ttest_rel #paired T-test\n",
    "from scipy.stats import levene #Levene's test\n",
    "\n",
    "#data cleansing tools\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "import missingno as mi\n",
    "\n",
    "#model building libraries and tools\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# --> CLASSIFIER MODELS\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from io import StringIO\n",
    "\n",
    "# --> REGRESSOR MODELS\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor)\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "#--> CLUSTERING\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "#use seaborn styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#Load Dataset\n",
    "\n",
    "dataset_filename = 'Violence-Project-Mass-Shooter-Database-Version-5-May-2022_r1.xlsx'\n",
    "print ('Dataset Filename :', dataset_filename)\n",
    "print ('')\n",
    "\n",
    "#data = pd.read_csv(dataset_filename) for .csv files\n",
    "\n",
    "data = pd.read_excel(dataset_filename, sheet_name = 'Full Database')\n",
    "\n",
    "# create copy of original data\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "#Determine the basic shape of the data - # of rows and # of cols\n",
    "\n",
    "print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string\n",
    "\n",
    "row_num = df.shape[0]\n",
    "print ('row_num =', row_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reduce Dataframe Memory Usage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## courtesy of https://www.kaggle.com/jeroenvdd TPSAPR22 Best non-DL model: tsflex\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open up view of columns and increase view of row data\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',250)\n",
    "\n",
    "#set a seed value to get the same results with each random sampling of the table\n",
    "\n",
    "np.random.seed(20)\n",
    "df.sample(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code snipet for quick review of categorical vars\n",
    "#obj_cols = ['col1_name', 'col2_name', 'col3_name', '.....']\n",
    "#df[obj_cols] = df[obj_cols].astype('category')\n",
    "\n",
    "\n",
    "# step through each category value and list the unique values in each category\n",
    "\n",
    "#for i in obj_cols:\n",
    "#    print('*'*40)\n",
    "#    print('Unique values for',i, 'are :')\n",
    "#    print(df[i].value_counts())\n",
    "#    print('*'*40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatype conversions\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting date columns to datetime\n",
    "#df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check conversion\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the missing data spatially in the dataset\n",
    "\n",
    "mi.matrix (df, figsize = (12,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for counts for obviously missing value counts\n",
    "\n",
    "df.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No obviously missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check  for the unique values\n",
    "# each row for each column\n",
    "n = df.nunique(axis=0)\n",
    "  \n",
    "print(\"No.of.unique values in each column :\\n\",\n",
    "     n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first glance at the variable data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df)[0:] #showing all columns\n",
    "df[columns].hist(stacked=False, bins=100, figsize=(40,64), layout=(15,2), color = 'orange'); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial insigths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id = \"link1\"></a> 1.  EDA - UNIVARIATE, BIVARIATE, and MULTIVARIATE ANALYSIS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build functions to increase EDA efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Numerical Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_boxplot(feature, figsize=(10,5), bins = None):\n",
    "   \n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows -> subplot grid= 2\n",
    "                                           sharex = True, # shared x-axis on both plots\n",
    "                                           gridspec_kw = {\"height_ratios\": (.15, .85)}, \n",
    "                                           figsize = figsize \n",
    "                                           ) # build 2 subplots\n",
    "    sns.boxplot(feature, ax=ax_box2, showmeans=True, color='lime') # boxplot will show the mean value\n",
    "    sns.distplot(feature, kde=T, ax=ax_hist2, bins=bins,color = 'red') if bins else sns.distplot(feature, kde=False, ax=ax_hist2,color='orange') # For histogram\n",
    "    ax_hist2.axvline(np.mean(feature), color='red', linestyle='--') # Add mean to histogram to help with assessing skewness\n",
    "    ax_hist2.axvline(np.median(feature), color='black', linestyle='-') # Add median to histogram to help with assessing skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical variables review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sl_No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df[\"Sl_No\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_vals = df[\"Sl_No\"].isnull().sum()\n",
    "print ('*'*100)\n",
    "print('')\n",
    "print ('obvious missing values to be treated', miss_vals)\n",
    "print('*'*100)\n",
    "df['Sl_No'].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unique index which will not add value to model building.  It has no statistical significance. Dropping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the Sl_No column \n",
    "df.drop('Sl_No',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df[\"Customer Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_vals = df[\"Customer Key\"].isnull().sum()\n",
    "print ('*'*100)\n",
    "print('')\n",
    "print ('obvious missing values to be treated', miss_vals)\n",
    "print('*'*100)\n",
    "df['Customer Key'].describe().T\n",
    "\n",
    "n_keys = df['Customer Key'].nunique()\n",
    "\n",
    "n_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Customer Key has 655 unique values out of 660 total identifiers.  It is acting as a category as well\n",
    "* There is no statistical value\n",
    "* Will ignore it when building Columns list for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the Customer Key column \n",
    "df.drop('Customer Key',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg_Credit_Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df[\"Avg_Credit_Limit\"])\n",
    "miss_vals = df[\"Avg_Credit_Limit\"].isnull().sum()\n",
    "print ('*'*100)\n",
    "print('')\n",
    "print ('obvious missing values to be treated', miss_vals)\n",
    "print('*'*100)\n",
    "df['Avg_Credit_Limit'].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data is right skewed with a significant number of outliers.\n",
    "* Customers have a wide range of credit limits, USD 3,000 - USD 200,000\n",
    "* On average, customer have a USD 34,574 Credit limit with 50% of the customers lying between USD 10,000 and USD 48,000.  The median Credit Limit is USD 18,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total_Credit_Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df[\"Total_Credit_Cards\"])\n",
    "miss_vals = df[\"Total_Credit_Cards\"].isnull().sum()\n",
    "print ('*'*100)\n",
    "print('')\n",
    "print ('obvious missing values to be treated', miss_vals)\n",
    "print('*'*100)\n",
    "df['Total_Credit_Cards'].describe().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Customers have at least 1 credit card, and up to 10 credit cards.  Data is left skewed, i.e., customers tend toward having fewer than more credit cards.  There are no outliers.  50% of the customers will have between 3 to 6 cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total_visits_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df[\"Total_visits_bank\"])\n",
    "miss_vals = df[\"Total_visits_bank\"].isnull().sum()\n",
    "print ('*'*100)\n",
    "print('')\n",
    "print ('obvious missing values to be treated', miss_vals)\n",
    "print('*'*100)\n",
    "df['Total_visits_bank'].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Customers tend toward more visits to the bank when they decide to make an in-person visit.  The data is right skewed.  Some never go and the Maximum number of visits is 5.  Half of all the customers will make between 1 and 4 trips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total_visits_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df[\"Total_visits_online\"])\n",
    "miss_vals = df[\"Total_visits_online\"].isnull().sum()\n",
    "print ('*'*100)\n",
    "print('')\n",
    "print ('obvious missing values to be treated', miss_vals)\n",
    "print('*'*100)\n",
    "df['Total_visits_online'].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When a customer chooses to contact the bank on-line, the average is 2.6 times. We don't know the frequency for these reported visits. Is this a monthly frequency, daily frequency, etc. There are several outliers.  Customers will range from not visiting on-line to a max visits on-line of 15. Half of the customers will visit on-line between 1 and 4 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total_calls_made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df[\"Total_calls_made\"])\n",
    "miss_vals = df[\"Total_calls_made\"].isnull().sum()\n",
    "print ('*'*100)\n",
    "print('')\n",
    "print ('obvious missing values to be treated', miss_vals)\n",
    "print('*'*100)\n",
    "df['Total_calls_made'].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When customers call in through the support line, they do so on average of 3.58 times during the time period in question.  Half of the customers will call in 1 to 5 times.  The data is right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary View of Numerical Distributions\n",
    "\n",
    "### Setup columns for clustering + visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore Customer Key, start with Col 2, collect columns that will be used for clustering\n",
    "all_col = df.iloc[:,:].columns.tolist()\n",
    "\n",
    "# building summary view of numerical values - HISTOGRAM\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20,10))\n",
    "fig.suptitle('Histogram of numerical variables', fontsize=20)\n",
    "counter = 0\n",
    "for ii in range(5):\n",
    "    sns.countplot(ax=axes[ii],x=df[all_col[counter]])\n",
    "    counter = counter+1\n",
    "\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary boxplots \n",
    "\n",
    "fig, axes = plt.subplots(1, 5,  figsize=(20, 6))\n",
    "fig.suptitle('Boxplot of numerical variables', fontsize=20)\n",
    "counter = 0\n",
    "for ii in range(5):\n",
    "    sns.boxplot(ax=axes[ii],x=df[all_col[counter]])\n",
    "    counter = counter+1\n",
    "\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select numerical columns\n",
    "fig, axes = plt.subplots(3, 2,  figsize=(20, 15))\n",
    "fig.suptitle('CDF plot of numerical variables', fontsize=20)\n",
    "counter = 0\n",
    "for ii in range(3):\n",
    "    sns.ecdfplot(ax=axes[ii][0],x=df[all_col[counter]])\n",
    "    counter = counter+1\n",
    "    if counter != 5:\n",
    "        sns.ecdfplot(ax=axes[ii][1],x=df[all_col[counter]])\n",
    "        counter = counter+1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Only about 10% of customers have a Credit Limit over USD 75,000.\n",
    "* 80% of the customers will have up to 6 credit cards.  Only 20% have more than that.\n",
    "* 75% of customers will choose to visit the bank in person at least 3 times.\n",
    "* 90% of customers will visit on-line at 5 times or less in a given time period.\n",
    "* About 80% of the customers will make 6 or fewer calls into the support line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build correlation matrix\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "sns.heatmap(df[all_col].corr(),\n",
    "            annot=True,\n",
    "            linewidths=0.5,vmin=-1,vmax=1,\n",
    "            center=0,\n",
    "            cbar=True,)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avg_Credit_Limit is mildly (positively) correlated with Total_Credit_Cards as we would expect. \n",
    "* The Total_calls_made is mildly (negatively) correlated with Total_Credit_Cards.\n",
    "* The rest are all weekly correlated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,13))\n",
    "sns.scatterplot(x='Total_Credit_Cards', y='Avg_Credit_Limit', data=df, palette ='colorblind');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for indicators of natural clustering counts\n",
    "sns.pairplot(df[all_col],diag_kind=\"kde\", corner = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* Early indicators for natural clusters based on modality in distribtions are:\n",
    "\n",
    "-- Avg_Credit_Limit - nothing obvious.\n",
    "\n",
    "-- Total_Credit_cards - 2\n",
    "\n",
    "-- Total_visits_bank - 1 to 2, nothing obvious.\n",
    "\n",
    "-- Same for visits on_line and calls_made.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id = \"link2\"></a> 2. Insights based on EDA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dataset does not contain a meaningful data dictionary to understand the features in a more nuanced fashion.  Given that we are trying to provide actional insights to Marketing and leadership to help improve customer perceptions in service, we need to get a better understanding of the time element for variables such as Total Visits to the bank in person, on-line, calls made to the support line.  It would also be good to have an understanding of the duration of those visits, calls, on-line sessions to understand how quickly a customer can gain resolution.  Or in this case, was the visit on-line or in-person for a problem to be resolved.  It is simply not clear from the amoutn of information given.\n",
    "\n",
    "* For the same of this EDA insights and later clustering and business recommendations, I will assume the following:\n",
    "\n",
    "- Average_Credit_Limit  : annual snapshot through any given year.  These credit limits suggest a mix of individuals and small-to-medium businesses.\n",
    "\n",
    "- Total_Credit_Cards :  this represents a single customer, who could be a consumer or a business owner that has multiple cards on the same account with the Average_Credit_Limit reported\n",
    "\n",
    "- In terms of visits, I will assume these numbers are annual as well as represent problem resolution visits through a specific channel....namely on-line, in person, and through a support call.\n",
    "\n",
    "** Please note that I would want to normally validate all of this with the key stakeholders **\n",
    "\n",
    "\n",
    "Customers have a wide range of credit limits, USD 3,000 - USD 200,000 suggesting a mix of personal and business banking. On average, customer have a USD 34,574 Credit limit with 50% of the customers lying between USD 10,000 and USD 48,000. The median Credit Limit is USD 18,000. The data has a significant number of outliers which are probably the handful of bsuiness customers.  This is all supported by Customers having at least 1 credit card (personal banking), and up to 10 credit cards (possibly a SMB). Data is left skewed, i.e., customers tend toward having fewer than more credit cards...again supporting the predominance of personal banking. 50% of the customers will have between 3 to 6 cards which might make sense if most of the customers were married and potentially had children who shared cards.  Unlikely that a bank is opening 3-6 discrete Credit Card accounts for a single customer.\n",
    "\n",
    "\n",
    "When customers choose to visit in-person the maximum number of visits is 5. Half of all the customers will make between 1 and 4 trips which seems very high.  This is assumed to be the most inconveniencing form of problem resolution or method to get answers to any queries given the time and energy to visit the bank.  It would be good to know the time to resolve when they do visit if we could gather that form the key stakeholders.\n",
    "\n",
    "When a customer chooses to contact the bank on-line, the average is about 2.6 times. Half of the customers will visit on-line between 1 and 4 times. Again these seem like high contact magnitudes and affected customer base, even in a year for problem resolution.  Some customers have had to visit up to 15 times.\n",
    "\n",
    "And when a customer has to call in to resolve an issue, it sometimes has taken up to 10 calls or they have had separate needs to call in up to 10 times in a given period.   We don't know specifically, but we know on average, customers make up to 3.58 calls and over half will make between 1 and 5 calls.\n",
    "\n",
    "Assuming again, these calls are all made in a period of 1 year, these numbers all seem very high based on personal experience in having to call into my bank to address questions or concerns with my own credit cards.  It is understandable that customers are frustrated and have a poor perception of the customer service.\n",
    "\n",
    "\n",
    "We do not have enough reasons to meaningfully address the \"Why?\" customers have for reaching out, but we can group them in the next section to explore patterns in behavior based on their number of cards and average credit limit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   <a id = \"link3\"></a> 3.  Data Pre-processing \n",
    "\n",
    "*  Scale the data set ahead of clustering\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "subset=df[all_col].copy()\n",
    "subset_scaled=scaler.fit_transform(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a dataframe out of the scaled data\n",
    "subset_scaled_df=pd.DataFrame(subset_scaled,columns=subset.columns)\n",
    "subset_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id = \"link4\"></a> 4. Clustering\n",
    "\n",
    "* Build clusters, k value selection, silhouette scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=range(1,9)\n",
    "meanDistortions=[]\n",
    "\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k)\n",
    "    model.fit(subset_scaled_df)\n",
    "    prediction=model.predict(subset_scaled_df)\n",
    "    distortion=sum(np.min(cdist(subset_scaled_df, model.cluster_centers_, 'euclidean'), axis=1)) / subset_scaled_df.shape[0]\n",
    "                           \n",
    "    meanDistortions.append(distortion)\n",
    "\n",
    "    print('Number of Clusters:', k, '\\tAverage Distortion:', distortion)\n",
    "\n",
    "plt.plot(clusters, meanDistortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Average Distortion')\n",
    "plt.title('Selecting k with the Elbow Method', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Elbow curve indicates k=4 is the most appropriate value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score = []\n",
    "cluster_list = list(range(2,10))\n",
    "for n_clusters in cluster_list:\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    preds = clusterer.fit_predict((subset_scaled_df))\n",
    "    #centers = clusterer.cluster_centers_\n",
    "    score = silhouette_score(subset_scaled_df, preds)\n",
    "    sil_score.append(score)\n",
    "    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cluster_list,sil_score)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(4, random_state = 1))\n",
    "visualizer.fit(subset_scaled_df)    \n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(3, random_state = 1))\n",
    "visualizer.fit(subset_scaled_df)    \n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting 3 as the appropriate level of clusters.  There is a solid Silhouette Score and this is also indicated as the inflection in the Elbow Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(subset_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['K_means_segments'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile = df.groupby('K_means_segments').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile['count_in_each_segment'] = df.groupby('K_means_segments')['Avg_Credit_Limit'].count().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5,  figsize=(16, 6))\n",
    "fig.suptitle('Boxplot of numerical variables for each cluster')\n",
    "counter = 0\n",
    "for ii in range(5):\n",
    "    sns.boxplot(ax=axes[ii],y=df[all_col[counter]],x=df['K_means_segments'])\n",
    "    counter = counter+1\n",
    "\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries needed for Hierarchial Clustering\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage,cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all linkage methods to check\n",
    "methods = ['single',\n",
    "           'average', \n",
    "           'complete',\n",
    "           'centroid',\n",
    "           'ward',\n",
    "          'weighted']\n",
    "\n",
    "# Calculate the pairwise distance form the dataset to be used in the cophenetic correlation calculation\n",
    "#pw_distance = pdist(credit_scaled)\n",
    "\n",
    "# Create lists to save results of coph calculation\n",
    "compare_cols = ['Linkage', 'Cophenetic Coefficient']\n",
    "# compare = []\n",
    "\n",
    "# Create a subplot image\n",
    "fig, axs = plt.subplots(len(methods), 1, figsize=(15, 30))\n",
    "\n",
    "# Enumerate through the list of all methods above\n",
    "# Get linkage, plot dendrogram, calculate cophenetic coefficient\n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "    Z = linkage(subset_scaled_df, metric='euclidean', method=method)\n",
    "\n",
    "    dendrogram(Z, ax=axs[i]);\n",
    "    axs[i].set_title(f'Dendrogram ({method.capitalize()} Linkage)')\n",
    "    coph_corr, coph_dist = cophenet(Z, pdist(subset_scaled_df))\n",
    "    axs[i].annotate(f'Cophenetic\\nCoefficient\\n{coph_corr:0.2f}', \n",
    "                    (0.80, 0.80),\n",
    "                    xycoords='axes fraction')\n",
    "#     compare.append([method, coph_corr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Ward Linkage is cleanest.  There are about 4 clusters.  Use 4 as starting point for AgglomerativeClustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying with K value as 4\n",
    "HCmodel = AgglomerativeClustering(n_clusters=4,affinity='euclidean', linkage='ward')\n",
    "HCmodel.fit(subset_scaled_df)\n",
    "subset_scaled_df['HC_Clusters'] = HCmodel.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HC_Clusters'] = HCmodel.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile = df.groupby('HC_Clusters').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize cluster profile\n",
    "cluster_profile.style.highlight_max(color = 'yellow', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5,  figsize=(16, 6))\n",
    "fig.suptitle('Boxplot of numerical variables for each cluster', fontsize=20)\n",
    "counter = 0\n",
    "for ii in range(5):\n",
    "    sns.boxplot(ax=axes[ii],y=df[all_col[counter]],x=df['HC_Clusters'])\n",
    "    counter = counter+1\n",
    "\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and comparison in KMeans Clustering and Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hierarchial Clustering suggested 4 clusters verus 3 from KMeans Clustering.\n",
    "* The 4th cluster contributed by Hierchial Clustering doesn't see to offer meaningfully different information than the 2nd cluster using the same technique.\n",
    "* In KMeans Clustering,  the 3 clusters were fairly distinct, suggesting a clear demarcation line for each of the features of consideration, from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id = \"link7\"></a> PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_scaled_df2 = subset_scaled_df.drop('HC_Clusters',axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(subset_scaled_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show variance explained by individual components\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulaize the Explained Individual Components\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--', color ='orange', lw = 3)\n",
    "plt.title(\"Explained Variances by Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.9,svd_solver ='full') # svd_solver -full helps to converge faster in case of very large data set\n",
    "pca.fit(subset_scaled_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.plot( pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--', color ='orange', lw = 3)\n",
    "plt.title(\"Explained Variances by Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_pca = pca.transform(subset_scaled_df2)\n",
    "subset_pca = pd.DataFrame(subset_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all linkage methods to check\n",
    "methods = ['single',\n",
    "           'average', \n",
    "           'complete',\n",
    "           'centroid',\n",
    "           'ward',\n",
    "          'weighted']\n",
    "\n",
    "# Calculate the pairwise distance form the dataset to be used in the cophenetic correlation calculation\n",
    "#pw_distance = pdist(credit_scaled)\n",
    "\n",
    "# Create lists to save results of coph calculation\n",
    "compare_cols = ['Linkage', 'Cophenetic Coefficient']\n",
    "# compare = []\n",
    "\n",
    "# Create a subplot image\n",
    "fig, axs = plt.subplots(len(methods), 1, figsize=(15, 30))\n",
    "\n",
    "# Enumerate through the list of all methods above\n",
    "# Get linkage, plot dendrogram, calculate cophenetic coefficient\n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "    Z = linkage(subset_pca, metric='euclidean', method=method)\n",
    "\n",
    "    dendrogram(Z, ax=axs[i]);\n",
    "    axs[i].set_title(f'Dendrogram ({method.capitalize()} Linkage)')\n",
    "    coph_corr, coph_dist = cophenet(Z, pdist(subset_scaled_df2))\n",
    "    axs[i].annotate(f'Cophenetic\\nCoefficient\\n{coph_corr:0.2f}', \n",
    "                    (0.80, 0.80),\n",
    "                    xycoords='axes fraction')\n",
    "#     compare.append([method, coph_corr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\n",
    "hc_labels = hc.fit_predict(subset_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dff = df.drop(['HC_Clusters'] ,axis=1)\n",
    "dff['PCA_HC_clusters'] = hc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile2 = dff.groupby('PCA_HC_clusters').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize second cluster profile 2\n",
    "cluster_profile2.style.highlight_max(color = 'pink', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5,  figsize=(16, 6))\n",
    "fig.suptitle('Boxplot of numerical variables for each cluster', fontsize=20)\n",
    "counter = 0\n",
    "for ii in range(5):\n",
    "    sns.boxplot(ax=axes[ii],y=dff[all_col[counter]],x=dff['PCA_HC_clusters'])\n",
    "    counter = counter+1\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id = \"link5\"></a> 6.  Actionable Insights & Business Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations from different types of Clustering as well as employing PCA followed by Clustering\n",
    "\n",
    "* There are 3 main clusters to review.   These comments for each cluster is based on the IQR values (namely, the behavior between quartiles 25% and 75%) representing typical behavior.  Extremes will be commented on as approporiate.  \n",
    "\n",
    "\n",
    "### Cluster 0\n",
    "\n",
    "* Customers have between ~ 12.5K and 55K USD in Average Credit Limit.  Notably, this group of customers have the minimum average credit limit and go up to 100K USD.\n",
    "* Customers tend to have between 4 and 6 credit cards, but could have as few as 2 and as many as 7.\n",
    "* They will typically make 3-5 in-person visits to the bank for query/problem resolution.\n",
    "* They will typcailly make 0-2 online visits, with a max of 3.\n",
    "* They will make 1-3 calls, with a maximum of 4.\n",
    "\n",
    "### Cluster 1\n",
    "\n",
    "* These customer have a very narrow Average Credit limit with outliers up to 50K USD. They tend to have what appears to be very low credit limits in general.  This could be customers who are new to building credit or customers repairing credit.\n",
    "\n",
    "* They tend to have between 1 and 3 cards and amaximum of 4 cards. Also indicating personal banking usage.\n",
    "\n",
    "* These customers tend to visit between 0 and 2 times in person, make 2-4 visits on-line, and make the most number of calls.  Their grand max is 10, and over half of them will make between 5 and 9 calls.\n",
    "\n",
    "### Cluster 2\n",
    "\n",
    "* This group of customers have the highest credit limits and number of cards typically.  The scale of the limits and number of cards suggest that these could be business accounts.\n",
    "\n",
    "* This group of customers make 0-1 trips per year and the fewest number of calls.  They prefer to bank on-line and ask/resolve their issues this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AllLife is focused on targeting new customers and upselling to exisiting customers through targeted marketing campaigns, with a tertiary goal of improving service levels to these customers.  In this approach, I have chosen the output from KMeans Clustering to suggest guidance.\n",
    "\n",
    "### Step 1:Customer Segmentation\n",
    "\n",
    "AllLife should segment their customers based on their stage in the credit journey characterized mainly by their Average Credit Limit and the number of cards they have/need for their accounts. (refer to specific Cluster Attributes above in this same section. I have chosen not to re-state them here.\n",
    "\n",
    "#### A. Established Customers  (Cluster 0 attributes)\n",
    "\n",
    "\n",
    "#### B. Credit Builders (Cluster 1 attributes)\n",
    "\n",
    "\n",
    "#### C. Business Customers (Cluster 2 attributes)\n",
    "\n",
    "\n",
    "### Step 2:  Target the customers you have for upsell activities & improving service quality perception\n",
    "\n",
    "Based on these characteristics described for each of the Cluster Attributes, marketing can quick identify specific customers in each segment and build a message around new products, as well as methods to share information based on their preferred engagement channel for questions/problem resolution.  They could also use this as an opportunity to earn trust with these customers, share improvements that have been made in these different channels to improve resolution time, as well as help improve the migration of customers who prefer walk-in and calls to using on-line resolution.  They may also consider enhancing their collateral describing these communication channels and how efficient they can be.  This will give the customer a feel that AllLife cares about their Experience each time they need support.\n",
    "\n",
    "### Step 3:  Target New Customers\n",
    "\n",
    "These Cluster Attributes and customer tendencies used in segmentation can be used to do another study of demographics, as well as tailor their outreach programs to include local advertising for customers looking to a bricks and mortar (in-person) experience versus those who engage for Credit Card services on-line.  They may consider techniques such as mailers, GoogleAds, account based marketing, as well as enhancing their websites based on SEO best practices.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}